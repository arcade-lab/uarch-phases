\section{Methodology}

We collected entire program profiles for nine different benchmarks. Eight of the benchmarks came from the MiBench~\cite{Guthaus:2001:MFC} benchmark suite, which covered applications in auto, office, network, security and telecommunication. The MiBench benchmarks measured includes \emph{basicmath, bitcount, qsort, stringsearch, dijkstra, sha, fft,} and \emph{ifft}. We made modifications to the input and output functionality to these benchmarks in order to run them as baremetal applications on an FPGA board. We also added matrix multiply (10x10 matrices) since it is a common kernel for several applications. We ran the MiBench benchmarks using the small input data set, which was of sufficient length to allow us to collect profiles in a reasonable amount of time while also long enough for detecting SFG phases, which was our primary goal.

All the benchmarks were run on a 32-bit LEON3 processor, which is based on the SPARC V8 RISC architecture. A LEON3 Verilog/VHDL customized core design was synthesized and ran on a Xilinx VC707 FPGA board. We added additional logic to the design in order to support tracing various core processor signals. The signals collected included the program counter, instruction type, processor stalls, bus activity, register file activity, and various other signals along the integer pipeline necessary for building the models and for analysis off-line. The signals were packed into 32-bits and stored in SRAMS ($\approx$4MB). When the SRAMS were filled up during runtime, an interrupt is generated and an interrupt handler reads and stores the samples to main memory. We looked at the overheads generated from the measurements and found that the highest overhead observed is 0.03\% for \textbf{basicmath}, which we believe to be reasonable. 

The PSD input parameters that are varied in our analysis includes the interval size, similarity threshold, and interval models. The minimum size of a phase is defined to be two intervals long here. The range for the interval size is 1-100K (cycles) and the similarity threshold range is between 0 and 1, where 0 indicates perfect similarity and 1 indicates not similar. The intervals are modeled as instruction working sets, basic block vectors, CPI, and also using the Intel Top-Down classification. The CPI is included as a reference point to assess how phases generated using IWS, BBV, and ITD compare to a purely performance driven model. We also only present averages across benchmarks here since we are primarily interested in analyzing the high-level trends in phase behavior that emerge as we vary these input parameters.

